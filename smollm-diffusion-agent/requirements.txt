# Core dependencies (all platforms)
torch>=2.0.0
transformers>=4.45.0
accelerate>=0.20.0
datasets>=2.12.0
scipy
sentencepiece
protobuf
wandb
hf_transfer

# CUDA-specific dependencies (install separately on Linux/Windows with NVIDIA GPU)
bitsandbytes>=0.41.0  # 4-bit quantization
flash-attn>=2.0.0  # FlashAttention-2 (2-3x speedup + 40-60% memory reduction)
triton>=2.0.0  # CUDA kernel compilation (required by flash-attn)
unsloth  # Faster training/inference
xformers  # Memory-efficient attention (alternative to flash-attn)
deepspeed  # Distributed training

# Optional (cross-platform)
optimum  # BetterTransformer (may conflict with quantization)

# Note: For Mac (MPS), use standard PyTorch with bfloat16
# Note: For MLX on Apple Silicon, see separate train_mlx.py
