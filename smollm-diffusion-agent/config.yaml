# Model Configuration
model:
  base_model_id: "HuggingFaceTB/SmolLM3-3B"
  use_unsloth: null  # null = auto-detect (CUDA only), true = force (CUDA only), false = disable
  enable_unsloth_inference_opt: true  # Enable unsloth for_inference() optimization (2x faster inference on CUDA)
  # Unsloth-specific optimizations (only used when use_unsloth=true)
  unsloth_use_gradient_checkpointing: "unsloth"  # "unsloth" = custom async offloading (30% VRAM save), true = standard, false = disabled
  unsloth_rope_scaling: null  # null = auto, or dict like {"type": "linear", "factor": 2.0} for longer contexts
  # Speed optimizations for base model (disable if using unsloth, as it has built-in optimizations)
  use_flash_attention: true  # Use FlashAttention-2 for base model (2-3x faster, 40-60% memory reduction on CUDA). Auto-disabled if unsloth is used or if import fails.
  use_gradient_checkpointing: true  # Enable gradient checkpointing (30-50% memory reduction, ~10% slower). Auto-disabled if unsloth is used.
  use_better_transformer: false  # Enable BetterTransformer (20-30% speedup, conflicts with quantization and unsloth)

# Quantization Configuration
# 4-bit quantization via bitsandbytes (CUDA only)
# On MPS/CPU: automatically disabled, falls back to bfloat16
quantization:
  enabled: true  # Enable 4-bit quantization on CUDA (auto-disabled on MPS/CPU)
  bits: 4  # Only 4-bit supported via bitsandbytes

# Diffusion Head Configuration
diffusion:
  num_steps: 4
  hidden_dim: 2048 # Should match base model hidden_size usually
  num_layers: 4    # Increased from 2 for better pattern learning
  num_heads: 8
  label_smoothing: 0.1  # Reduces overconfidence, improves generalization
  use_bidirectional: true  # Use bidirectional attention for global constraint verification
  use_optimized_attention: true  # Use SDPA-optimized attention (2-3x faster, FlashAttention on CUDA)
  use_attention_mask: false  # Avoids padding attention during training
  null_loss_weight: 0.1  # Down-weight NULL labels to avoid NULL-dominant solutions
  null_prediction_penalty: 0.3  # Penalize NULL predictions on non-NULL labels
  entropy_weight: 0.025  # Reduced from 0.07: high entropy causes random garbage, lower value stabilizes
  training_temperature: 1.2  # Reduced from 1.5: less smoothing to reduce low-confidence token spam
  repetition_penalty: 0.0  # Disabled by default (masked positions are non-contiguous)

# Training Configuration
# Optimized for 4x RTX 4090 (24GB each) with 4-bit quantization
# Effective batch size = batch_size * num_gpus * gradient_accumulation_steps = 4 * 4 * 4 = 64
# Note: batch_size 6 OOMs due to diffusion head output_proj ([batch, seq, vocab=128K] tensor)
training:
  batch_size: 4
  learning_rate: 1.2e-4  # Slightly increased for larger effective batch (384 vs previous 32-48)
  optimizer:
    name: "muon"
    muon:
      lr: 1.2e-4
      momentum: 0.95
      weight_decay: 0.01
    adamw:
      lr: 1.2e-4
      betas: [0.9, 0.95]
      eps: 1.0e-08
      weight_decay: 0.01
  scheduler:
    name: "one_cycle"
    max_lr: 1.2e-4
    pct_start: 0.1
    anneal_strategy: "cos"
    div_factor: 25.0
    final_div_factor: 10000.0
    cycle_momentum: false
    three_phase: false
  num_epochs: 6  # Reduced from 8: larger effective batch (384) converges faster, 6 epochs sufficient
  gradient_accumulation_steps: 4
  max_seq_len: 2048  # median=556, 90th percentile=1270, 95th percentile=1742
  max_new_tokens: 256
  seed: 42
  use_wandb: true
  wandb_project: "smollm-diffusion-agent"
  logits_for_metrics: true
  resume_from_checkpoint: false
  checkpoint_path: "checkpoints/best_model/model.pt"
  eval_num_samples: 10  # Number of samples to evaluate on
  warmup_steps: 300  # Short warmup; gradient_accum makes batch-step progress faster than optimizer steps
  min_lr_ratio: 0.01  # Minimum LR as fraction of peak (0.01 = 1% of learning_rate, e.g., 1.2e-6)
  ddp_find_unused_parameters: false
  compile:
    enabled: false          # Enable torch.compile for training (CUDA only, auto-disabled on MPS/CPU)
    mode: "reduce-overhead" # Compilation mode: default, reduce-overhead, or max-autotune
    fullgraph: false        # Keep false for stability with dynamic control flow

# Inference Configuration
inference:
  use_torch_compile: false  # Enable torch.compile for diffusion head (CUDA only, auto-disabled on MPS/CPU)
  use_cuda_graph: true     # Enable CUDA graphs for diffusion head (CUDA only, auto-disabled on MPS/CPU)
  steps: 4                 # Number of diffusion denoising steps
  temperature: 0.0         # Sampling temperature (0.0 = greedy)
  max_seq_len: 2048        # Maximum sequence length for inference
  # Running Confidence Remasking (RCR) - allows token revision during inference
  remasking:
    enabled: true          # Enable RCR to fix over-denoising and allow token revision
    remask_ratio: 0.2      # Fraction of lowest-confidence revealed tokens to remask each step
    min_lock_confidence: 0.7  # Minimum running confidence to keep a token locked
  expansion:
    enabled: true
    max_rounds: 2
    expand_tokens: 4
    tail_window: 4
    tail_null_threshold: 0.5

# Data Configuration
data:
  # Single dataset mode (backwards compatible):
  # dataset_name: "interstellarninja/hermes_reasoning_tool_use"
  
  # Multi-dataset mode (recommended):
  datasets:
    - name: "interstellarninja/hermes_reasoning_tool_use"
      split: "train"
      weight: 1.0  # Use full dataset
      limit: null
    - name: "Salesforce/xlam-function-calling-60k"
      split: "train"
      weight: 1.0  # Use full dataset
      limit: null
  
  limit: null # Global limit (overrides individual dataset limits if set)
  mask_budget: 32  # Max tokens per field (cap), used for training + inference
  dynamic_budget:
    enabled: true
    min_tokens: 0
    extra_tokens: 2
    max_tokens: 32
  mask_token: null # null = use <|reserved_special_token_2|> (recommended), "eos" = use EOS, or specify token string
  null_token: null # null = use <|reserved_special_token_11|> for variable-length fields (automatic budgeting)
  system_message: "/no_think"  # SmolLM3 chat_template uses /think or /no_think in system message
  max_history_messages: 12     # Keep last N chat messages before the tool call to fit max_seq_len
  chat_sampling_rate: 0.1  # Sampling rate for chat examples (non-tool calls). 0.1 = 10% of chat turns
  bucket_sizes: [ 512, 1024, 1280, 1536, 1792, 2048 ]  # Buckets align with 90th/95th percentiles
  cache:
    enabled: true
    dir: "data_cache"

