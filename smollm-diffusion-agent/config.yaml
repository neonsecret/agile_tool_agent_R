# Model Configuration
model:
  base_model_id: "HuggingFaceTB/SmolLM3-3B"
  mlx_base_model_id: "mlx-community/SmolLM3-3B-4bit"
  hidden_size: 2048 # Will be auto-detected but good to have default
  vocab_size: 32000
  backend: "pytorch"  # "pytorch" or "mlx"
  use_unsloth: null  # null = auto-detect (use unsloth on CUDA if available), true/false to override
  enable_unsloth_inference_opt: true  # Enable unsloth's for_inference() optimization (2x faster inference, safe for frozen base models)

# Quantization Configuration (independent of backend)
# This setting applies to both PyTorch and MLX backends
quantization:
  enabled: true  # Enable quantization (works for both PyTorch and MLX)
  bits: 4  # Quantization bits: 4 or 8
  # Backend-specific behavior:
  #   - PyTorch: Only 4-bit supported (via bitsandbytes, CUDA only)
  #              If bits != 4 or not on CUDA, falls back to bfloat16
  #   - MLX: Supports 4-bit and 8-bit natively on Apple Silicon
  #          If enabled: false, loads full precision model

# Diffusion Head Configuration
diffusion:
  num_steps: 4
  hidden_dim: 2048 # Should match base model hidden_size usually
  num_layers: 2
  num_heads: 8
  label_smoothing: 0.1  # Reduces overconfidence, improves generalization
  use_bidirectional: true  # Use bidirectional attention for global constraint verification

# Training Configuration
training:
  batch_size: 4
  learning_rate: 1.0e-4
  num_epochs: 4
  gradient_accumulation_steps: 2
  train_router: true  # Enable router training (Chat=0, Tool=1, Think=2)
  max_seq_len: 2048  # Increased from 1024: median=1198, 90th percentile=2278
  max_new_tokens: 256
  seed: 42
  use_wandb: true
  wandb_project: "smollm-diffusion-agent"
  resume_from_checkpoint: true
  checkpoint_path: "checkpoints/best_model/model.pt"
  eval_every_n_steps: 1000  # Run functional evaluation every N steps
  eval_num_samples: 10  # Number of samples to evaluate on
  warmup_steps: 2500  # LR warmup steps (from guide.md), capped at 10% of total steps
  # Distributed training (MLX only)
  distributed:
    enabled: false
    num_machines: 1
    num_processes_per_machine: 1
    backend: "ring"  # "ring" or "mpi"
    main_process_port: 29500
  compile:
    enabled: true          # Enable torch.compile for training
    mode: "reduce-overhead" # Reduces CPU overhead and can auto-graph safe regions
    fullgraph: false        # Keep false for stability with dynamic pieces

# Inference Configuration
inference:
  use_kv_cache: true       # Reuse base-LLM KV for prompt across diffusion steps
  use_torch_compile: false  # Enable torch.compile for diffusion head (CUDA only; falls back if unsupported)
  use_cuda_graph: true     # Enable CUDA graphs for diffusion head (requires fixed shapes; falls back if mismatch)
  steps: 4
  temperature: 0.0
  cfg_scale: 0.0

# Data Configuration
data:
  dataset_name: "interstellarninja/hermes_reasoning_tool_use"
  limit: null # Set to null for full dataset, integer for debugging
  mask_budget: 48
  mask_token: null # null = use <|reserved_special_token_2|> (recommended), "eos" = use EOS, or specify token string
  null_token: null # null = use <|reserved_special_token_11|> for variable-length fields (automatic budgeting)
  chat_sampling_rate: 0.1  # Sampling rate for chat examples (router_label=0). 0.1 = 10% of chat turns
  bucket_sizes: [512, 1024, 1536, 2048]  # Fixed buckets for padding/compile stability

