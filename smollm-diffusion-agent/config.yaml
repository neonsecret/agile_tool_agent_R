# Model Configuration
model:
  base_model_id: "HuggingFaceTB/SmolLM3-3B"
  load_in_4bit: true
  hidden_size: 2048 # Will be auto-detected but good to have default
  vocab_size: 32000

# Diffusion Head Configuration
diffusion:
  num_steps: 4
  hidden_dim: 2048 # Should match base model hidden_size usually
  num_layers: 2
  num_heads: 8

# Training Configuration
training:
  batch_size: 4
  learning_rate: 1.0e-4
  num_epochs: 4
  gradient_accumulation_steps: 2
  train_router: false
  max_seq_len: 2048  # Increased from 1024: median=1198, 90th percentile=2278
  max_new_tokens: 256
  seed: 42
  use_wandb: true
  wandb_project: "smollm-diffusion-agent"
  resume_from_checkpoint: false
  checkpoint_path: "checkpoints/best_model/model.pt"
  eval_every_n_steps: 1000  # Run functional evaluation every N steps
  eval_num_samples: 10  # Number of samples to evaluate on

# Data Configuration
data:
  dataset_name: "interstellarninja/hermes_reasoning_tool_use"
  limit: null # Set to null for full dataset, integer for debugging
  mask_budget: 48
  mask_token: null # null = use <|reserved_special_token_2|> (recommended), "eos" = use EOS, or specify token string
  chat_sampling_rate: 0.1  # Sampling rate for chat examples (router_label=0). 0.1 = 10% of chat turns

