# Model Configuration
model:
  base_model_id: "HuggingFaceTB/SmolLM3-3B"
  hidden_size: 2048  # Auto-detected from model, kept for reference
  vocab_size: 32000  # Auto-detected from tokenizer, kept for reference
  use_unsloth: null  # null = auto-detect (CUDA only), true = force (CUDA only), false = disable
  enable_unsloth_inference_opt: true  # Enable unsloth for_inference() optimization (2x faster inference on CUDA)

# Quantization Configuration
# 4-bit quantization via bitsandbytes (CUDA only)
# On MPS/CPU: automatically disabled, falls back to bfloat16
quantization:
  enabled: true  # Enable 4-bit quantization on CUDA (auto-disabled on MPS/CPU)
  bits: 4  # Only 4-bit supported via bitsandbytes

# Diffusion Head Configuration
diffusion:
  num_steps: 4
  hidden_dim: 2048 # Should match base model hidden_size usually
  num_layers: 2
  num_heads: 8
  label_smoothing: 0.1  # Reduces overconfidence, improves generalization
  use_bidirectional: true  # Use bidirectional attention for global constraint verification

# Training Configuration
training:
  batch_size: 8
  learning_rate: 1.0e-4
  num_epochs: 4
  gradient_accumulation_steps: 2
  train_router: true  # Enable router training (Chat=0, Tool=1, Think=2)
  max_seq_len: 2048  # Increased from 1024: median=1198, 90th percentile=2278
  max_new_tokens: 256
  seed: 42
  use_wandb: true
  wandb_project: "smollm-diffusion-agent"
  resume_from_checkpoint: false
  checkpoint_path: "checkpoints/best_model/model.pt"
  eval_every_n_steps: 1000  # Run functional evaluation every N steps
  eval_num_samples: 10  # Number of samples to evaluate on
  warmup_steps: 2500  # LR warmup steps (from guide.md), capped at 10% of total steps
  # Distributed training (MLX only)
  distributed:
    enabled: false
    num_machines: 1
    num_processes_per_machine: 1
    backend: "ring"  # "ring" or "mpi"
    main_process_port: 29500
  compile:
    enabled: false          # Enable torch.compile for training (CUDA only, auto-disabled on MPS/CPU)
    mode: "reduce-overhead" # Compilation mode: default, reduce-overhead, or max-autotune
    fullgraph: false        # Keep false for stability with dynamic control flow

# Inference Configuration
inference:
  use_torch_compile: false  # Enable torch.compile for diffusion head (CUDA only, auto-disabled on MPS/CPU)
  use_cuda_graph: true     # Enable CUDA graphs for diffusion head (CUDA only, auto-disabled on MPS/CPU)
  steps: 4                 # Number of diffusion denoising steps
  temperature: 0.0         # Sampling temperature (0.0 = greedy)
  cfg_scale: 0.0           # Classifier-free guidance scale
  max_seq_len: 2048        # Maximum sequence length for inference

# Data Configuration
data:
  dataset_name: "interstellarninja/hermes_reasoning_tool_use"
  limit: null # Set to null for full dataset, integer for debugging
  mask_budget: 48
  mask_token: null # null = use <|reserved_special_token_2|> (recommended), "eos" = use EOS, or specify token string
  null_token: null # null = use <|reserved_special_token_11|> for variable-length fields (automatic budgeting)
  system_message: "/no_think"  # SmolLM3 chat_template uses /think or /no_think in system message
  max_history_messages: 12     # Keep last N chat messages before the tool call to fit max_seq_len
  chat_sampling_rate: 0.1  # Sampling rate for chat examples (router_label=0). 0.1 = 10% of chat turns
  bucket_sizes: [512, 1024, 1536, 2048]  # Fixed buckets for padding/compile stability

