# Model Configuration
model:
  base_model_id: "HuggingFaceTB/SmolLM3-3B"
  use_unsloth: null  # null = auto-detect (CUDA only), true = force (CUDA only), false = disable
  enable_unsloth_inference_opt: true  # Enable unsloth for_inference() optimization (2x faster inference on CUDA)
  # Unsloth-specific optimizations (only used when use_unsloth=true)
  unsloth_use_gradient_checkpointing: "unsloth"  # "unsloth" = custom async offloading (30% VRAM save), true = standard, false = disabled
  unsloth_rope_scaling: null  # null = auto, or dict like {"type": "linear", "factor": 2.0} for longer contexts
  # Speed optimizations for base model (disable if using unsloth, as it has built-in optimizations)
  use_flash_attention: true  # Use FlashAttention-2 for base model (2-3x faster, 40-60% memory reduction on CUDA). Auto-disabled if unsloth is used or if import fails.
  use_gradient_checkpointing: true  # Enable gradient checkpointing (30-50% memory reduction, ~10% slower). Auto-disabled if unsloth is used.
  use_better_transformer: false  # Enable BetterTransformer (20-30% speedup, conflicts with quantization and unsloth)

# Quantization Configuration
# 4-bit quantization via bitsandbytes (CUDA only)
# On MPS/CPU: automatically disabled, falls back to bfloat16
quantization:
  enabled: true  # Enable 4-bit quantization on CUDA (auto-disabled on MPS/CPU)
  bits: 4  # Only 4-bit supported via bitsandbytes

# Diffusion Head Configuration
diffusion:
  num_steps: 4
  hidden_dim: 2048 # Should match base model hidden_size usually
  num_layers: 4    # Increased from 2 for better pattern learning
  num_heads: 8
  label_smoothing: 0.1  # Reduces overconfidence, improves generalization
  use_bidirectional: true  # Use bidirectional attention for global constraint verification
  use_optimized_attention: true  # Use SDPA-optimized attention (2-3x faster, FlashAttention on CUDA)
  use_attention_mask: false  # Avoids padding attention during training
  init_vocab_from_base: true  # Initialize token_emb/output_proj from base model (trainable)
  null_loss_weight: 0.35  # Down-weight NULL labels to avoid NULL-dominant solutions
  null_prediction_penalty: 0.3  # Penalize NULL predictions on non-NULL labels
  entropy_weight: 0.08  # Higher entropy to reduce collapse
  training_temperature: 1.0  # No temperature smoothing during training
  repetition_penalty: 0.0  # Disabled by default (masked positions are non-contiguous)
  t_sampling: "mixture_high"  # uniform | sqrt | mixture_high
  t_high_prob: 0.3
  t_high_range: [0.8, 1.0]
  use_prompt_cross_attention: true
  prompt_cross_attention_heads: null
  use_field_position: true
  field_position_max_len: 32

# Training Configuration
# Optimized for 4x RTX 4090 (24GB each) with 4-bit quantization
# Effective batch size = batch_size * num_gpus * gradient_accumulation_steps = 4 * 4 * 4 = 64
# Note: batch_size 6 OOMs due to diffusion head output_proj ([batch, seq, vocab=128K] tensor)
training:
  batch_size: 6
  learning_rate: 2.0e-5  # Lower LR for post-training (was 1.2e-4 for initial training)
  optimizer:
    name: "muon"
    muon:
      lr: 0.001  # Reduced for post-training (was 0.005)
      momentum: 0.95
      weight_decay: 0.01
    adamw:
      lr: 2.0e-5  # Reduced for post-training (was 1.2e-4)
      betas: [0.9, 0.95]
      eps: 1.0e-08
      weight_decay: 0.01
  scheduler:
    name: "cosine"  # Cosine for post-training (was one_cycle for initial training)
    max_lr: [0.001, 2.0e-5]  # [muon_group, adamw_group] - reduced for post-training
    pct_start: 0.1
    use_warmup_steps: true
    anneal_strategy: "cos"
    div_factor: 25.0
    final_div_factor: 10000.0
    cycle_momentum: false
    three_phase: false
  num_epochs: 4  # Short post-training run (was 6 for initial training)
  gradient_accumulation_steps: 4
  max_seq_len: 2048  # median=556, 90th percentile=1270, 95th percentile=1742
  max_new_tokens: 256
  seed: 42
  use_wandb: true
  wandb_project: "smollm-diffusion-agent-posttrain"  # Different project for post-training
  logits_for_metrics: true
  resume_from_checkpoint: true  # Set to true for post-training
  checkpoint_path: "checkpoints/best_model/model.pt"
  load_weights_only: true  # Reset epoch/optimizer/scheduler, keep weights only
  eval_num_samples: 20  # Number of samples to evaluate on
  warmup_steps: 100  # Shorter warmup for post-training (was 200)
  min_lr_ratio: 0.01  # Minimum LR as fraction of peak (0.01 = 1% of learning_rate, e.g., 1.2e-6)
  ddp_find_unused_parameters: false
  compile:
    enabled: false          # Enable torch.compile for training (CUDA only, auto-disabled on MPS/CPU)
    mode: "reduce-overhead" # Compilation mode: default, reduce-overhead, or max-autotune
    fullgraph: false        # Keep false for stability with dynamic control flow

# Inference Configuration
inference:
  use_torch_compile: false  # Enable torch.compile for diffusion head (CUDA only, auto-disabled on MPS/CPU)
  use_cuda_graph: true     # Enable CUDA graphs for diffusion head (CUDA only, auto-disabled on MPS/CPU)
  steps: 4                 # Number of diffusion denoising steps
  temperature: 0.0         # Sampling temperature (0.0 = greedy)
  max_seq_len: 2048        # Maximum sequence length for inference
  reencode_hidden_states_every: 1  # 0 = never, 1 = every step, 2 = every 2 steps, etc.
  budget_from_base_tool_call: false
  # Running Confidence Remasking (RCR) - allows token revision during inference
  remasking:
    enabled: true          # Enable RCR to fix over-denoising and allow token revision
    remask_ratio: 0.2      # Fraction of lowest-confidence revealed tokens to remask each step
    min_lock_confidence: 0.7  # Minimum running confidence to keep a token locked
  expansion:
    enabled: true
    max_rounds: 2
    expand_tokens: 4
    tail_window: 4
    tail_null_threshold: 0.5

# Data Configuration
data:
  # Single dataset mode (backwards compatible):
  # dataset_name: "interstellarninja/hermes_reasoning_tool_use"
  
  # Multi-dataset mode (recommended):
  # Distribution: 70% general function calling, 30% code/math-heavy
  datasets:
    # General function calling datasets (70% of total) - UNLIMITED
    - name: "interstellarninja/hermes_reasoning_tool_use"
      split: "train"
      weight: 1.0
      limit: null  # ~20k examples
    - name: "Salesforce/xlam-function-calling-60k"
      split: "train"
      weight: 1.0
      limit: null  # ~60k examples
    - name: "glaiveai/glaive-function-calling-v2"
      split: "train"
      weight: 0.5
      limit: 50000  # ~25k effective
    - name: "Salesforce/APIGen-MT-5k"
      split: "train"
      weight: 1.0
      limit: null  # ~5k examples
    - name: "nvidia/When2Call"
      split: "train"
      weight: 0.5
      limit: null  # ~7.5k effective (from train_sft subset)
    
    # Code/Math-heavy datasets (30% of total) - LIMITED
    - name: "argilla/apigen-function-calling"
      split: "train"
      weight: 1.0
      limit: 48500  # CODE-HEAVY: ~48.5k for 30% ratio
    - name: "ibm-research/nestful"
      split: "train"
      weight: 1.0
      limit: null  # MATH/CODE: ~1.86k (all examples)
  
  limit: null # Global limit (overrides individual dataset limits if set)
  mask_budget: 32  # Max tokens per field (cap), used for training + inference
  dynamic_budget:
    enabled: true
    min_tokens: 0
    extra_tokens: 2
    max_tokens: 32
    # Length jitter for teaching robust length control
    # Modes: "over" (add tokens), "under" (remove tokens), "both" (random)
    length_jitter:
      enabled: true
      mode: "both"  # over-budget AND under-budget jitter for full robustness
      min_jitter: 0
      max_jitter: 5
  mask_token: null # null = use <|reserved_special_token_2|> (recommended), "eos" = use EOS, or specify token string
  null_token: null # null = use <|reserved_special_token_11|> for variable-length fields (automatic budgeting)
  system_message: "/no_think"  # SmolLM3 chat_template uses /think or /no_think in system message
  max_history_messages: 12     # Keep last N chat messages before the tool call to fit max_seq_len
  chat_sampling_rate: 0.1  # Sampling rate for chat examples (non-tool calls). 0.1 = 10% of chat turns
  bucket_sizes: [ 512, 1024, 1280, 1536, 1792, 2048 ]  # Buckets align with 90th/95th percentiles
  cache:
    enabled: true
    dir: "data_cache"

