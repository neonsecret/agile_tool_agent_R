Here is a comprehensive structural guide based on the provided document. It is organized as a technical specification and implementation roadmap for building a State-of-the-Art (SOTA) small language model (3B parameters) specialized in function calling and reasoning.

---

# Project Guide: Hybrid Diffusion-Autoregressive Architecture for Function Calling
**Target Model:** ~3B Parameters (Based on SmolLM3)
**Core Goal:** Create a small, deployable agent that excels at structured function calling (via Diffusion) while maintaining high-quality conversation and reasoning (via Autoregression).

## 1. Executive Summary & Core Concept
The architecture rejects the "one size fits all" approach. It uses a **Dual-Mode Hybrid system** where different computational mechanisms handle different tasks.

*   **Conversation & Reasoning:** handled by the **Autoregressive (AR)** base model (SmolLM3).
*   **Function Calling:** handled by a **Schema-Constrained Diffusion Head** using a "Scaffolding" technique.
*   **Routing:** A lightweight classifier decides which mode to activate per user query.

**Why this wins:**
1.  **AR** is fast for chat.
2.  **Diffusion** allows bidirectional attention, ensuring global consistency in JSON structures (solving syntax errors).
3.  **Scaffolding** (generated by Python, not the LLM) guarantees 0% structural hallucination.

---

## 2. Key Research & Literature (The "Why")
These papers provide the theoretical foundation for the architectural choices.

### **A. Function Calling Optimization**
*   **Source:** [MediaTek - Enhancing Function-Calling (arXiv:2412.01130v2)](https://arxiv.org/html/2412.01130v2)
*   **Key Takeaway:** Use a **Decision Token** (binary classification `<|answer|>` vs `<|use_tool|>`) before generating.
*   **Stat:** Improves relevance detection from 49.58% → 65.42%.
*   **Instruction:** Do *not* use Chain-of-Thought (CoT) for function calling; it adds noise. Use CoT only for math/logic.

### **B. Structured Generation via Diffusion**
*   **Source:** [Unveiling Potential of Diffusion LLMs (arXiv:2507.04504)](https://arxiv.org/pdf/2507.04504.pdf)
*   **Key Takeaway:** **Schema Scaffolding**. Instead of generating raw JSON, generate a template with masks.
*   **Stat:** 65% improvement in structural adherence; 17% reduction in hallucination.
*   **Technique:** Bidirectional attention verifies constraints simultaneously, unlike AR's sequential guessing.

### **C. Base Model Architecture**
*   **Source:** [SmolLM3 Blog](https://huggingface.co/blog/smollm3)
*   **Key Takeaway:** Already supports dual-mode (`think` vs `no_think`).
*   **Strategy:** Use its existing reasoning capabilities; do not retrain reasoning from scratch, but include reasoning data to prevent "catastrophic forgetting."

### **D. Relevant Architectures (For Code Inspiration)**
*   **LaDiR (Latent Diffusion Reasoning):** [GitHub](https://github.com/mk322/LaDiR). *Use for diffusion head code structure, but avoid the VAE/Latent space complexity.*
*   **Masked Diffusion LM (MDLM):** [GitHub](https://github.com/s-sahoo/mdlm). *Use for the masking loss objective.*
*   **HybridVLA:** [Paper](https://arxiv.org/pdf/2503.10631). *Proof that AR+Diffusion hybrids work.*

---

## 3. The Architecture: "LaFuNC" (Latent Function Calling)

### **The Flow**
1.  **Input:** User Query + Context.
2.  **Stage 1: Mode Router (AR Head)**
    *   Predicts: `<|chat|>`, `<|think|>`, or `<|tool|>`.
    *   Latency: <10ms.
    *   **Architecture:** Lightweight 3-way classifier using a simple linear layer on top of the base model's hidden states. Takes the last token representation and maps to three mode classes.
3.  **Stage 2: Execution (Mode Dependent)**
    *   **If Chat:** Standard AR generation.
    *   **If Think:** AR generation with `<|thinking|>` traces (CoT).
    *   **If Tool:**
        *   **A. Decision:** AR Model predicts function name (e.g., `get_weather`).
        *   **B. Scaffold (Python):** Python script looks up schema and builds template: `{"loc": "<MASK>", "unit": "<MASK>"}`.
        *   **C. Diffusion (Head):** Model predicts *only* the masked tokens (2-4 steps).

### **Detailed Execution Flow: The "Handshake" Between Components**

**Critical Understanding:** The template is generated by Python code (deterministic), NOT by the LLM. The LLM is terrible at syntax (brackets, commas). Python is perfect at syntax. We play to their strengths.

**Step-by-Step Execution for a Single Request:**

**Example Query:** "What's the weather in Tokyo?"

1.  **Stage 1 - The "Brain" (Autoregressive Model):**
    *   SmolLM3 processes the query in AR mode.
    *   Generates decision token: `<|decision:use_tool|>`
    *   Selects function name: `<|tool_name:get_weather|>`
    *   **Execution pauses here.**

2.  **Stage 2 - The "Constructor" (Python Script):**
    *   Python inference code reads the token `<|tool_name:get_weather|>`.
    *   Looks up the schema definition for `get_weather` from the function registry.
    *   Schema example: `{"name": "get_weather", "parameters": {"location": {"type": "string"}, "unit": {"type": "string", "enum": ["C", "F"]}}}`
    *   **Python generates the Scaffold:** Builds a string where structure is hard-coded and values are replaced with `<MASK>` tokens.
    *   Generated scaffold: `{"location": "<MASK>", "unit": "<MASK>"}`
    *   This scaffold is tokenized and fed to the diffusion head.

3.  **Stage 3 - The "Painter" (Diffusion Head):**
    *   **Input:** User query embedding + Scaffold embedding (with mask positions marked).
    *   **Action:** Diffusion model does NOT generate the keys like `"location":`. It ONLY predicts tokens to replace `<MASK>`.
    *   **Context Awareness:** Uses bidirectional attention to see "Tokyo" in context while simultaneously seeing the "location" slot needs filling.
    *   **Iterative Refinement:** Runs 2-4 denoising steps. Each step refines the prediction, keeping 70% of previous predictions and updating 30% with new predictions.
    *   **Diffusion Output:** `"Tokyo"`, `"C"`

4.  **Stage 4 - Final Assembly (Python Script):**
    *   Python script merges the deterministic scaffold with diffusion outputs.
    *   **Final JSON:** `{"location": "Tokyo", "unit": "C"}`
    *   This is guaranteed to be valid JSON because Python built the structure.

**Why This Specific Flow is SOTA:**
*   **0% Syntax Errors:** JSON is guaranteed valid because Python built it.
*   **0% Schema Hallucination:** Model cannot invent parameters because there's no `<MASK>` slot for them in the scaffold.
*   **High Accuracy Values:** Diffusion model focuses 100% of compute on extracting "Tokyo", not wasting compute predicting the letters "l-o-c-a-t-i-o-n".

### **Component Breakdown**

| Component | Type | Function | Training |
| :--- | :--- | :--- | :--- |
| **Base Model** | SmolLM3-3B | Understands context, routing, reasoning. | **FROZEN** (mostly) |
| **Router Head** | Linear Layer | Classifies Chat vs. Tool vs. Think. | Train from scratch |
| **Diffusion Head** | MLP/Transformer | Fills `<MASK>` slots in scaffolds. | Train from scratch |
| **Python Logic** | Script | Deterministically builds JSON structure. | Deterministic Code |

---

## 4. Step-by-Step Implementation Guide

### **Phase 0: Setup & Base Model**
*   **Model:** Start with **SmolLM3-3B-Instruct**.
*   **Repo:** Create a new repository (`smollm-diffusion-agent`). Do not fork complex pre-training libraries (Nanotron). Build a wrapper class.

**Why NOT Fork the SmolLM Repo:**
*   The official SmolLM training repositories are engineered for massive-scale standard pre-training and SFT using frameworks like `nanotron` or `alignment-handbook`.
*   These are optimized for standard training pipelines and do not support custom architectures with multiple heads (AR + Diffusion) and custom loss functions.
*   Attempting to integrate diffusion logic into their optimized pipelines will create maintenance nightmares.
*   **Solution:** Treat SmolLM3 as a library component. Build a wrapper architecture that loads the pretrained model and attaches your custom heads.

**Project Structure:**
```
smollm-diffusion-agent/
├── data/
│   ├── generate_scaffolds.py    # Converts datasets into scaffold format
│   └── dataset_loader.py        # HuggingFace dataset wrapper
├── model/
│   ├── __init__.py
│   ├── diffusion_head.py        # Schema-constrained diffusion head
│   └── hybrid_model.py          # Wrapper combining SmolLM + Diffusion
├── train.py                     # Training loop with custom losses
└── inference.py                 # AR → Scaffold → Diffusion pipeline
```

### **Phase 1: The Wrapper Architecture**
Create `model/hybrid_model.py`. This class wraps the frozen base model and attaches the trainable diffusion head.

```python
class HybridSmolLM(nn.Module):
    def __init__(self):
        # Load Frozen SmolLM3
        self.base_llm = AutoModelForCausalLM.from_pretrained("SmolLM3-3B")
        for param in self.base_llm.parameters(): param.requires_grad = False
        
        # Add Trainable Diffusion Head
        self.diffusion_head = SchemaDiffusionHead(...)

    def forward(self, inputs, scaffold_mask):
        # Get hidden states from Base LLM
        hidden = self.base_llm(inputs).hidden_states[-1]
        # Pass masked positions to Diffusion Head
        return self.diffusion_head(hidden, scaffold_mask)
```

### **Phase 2: Data Pipeline (The Scaffolder)**
Write `data/generate_scaffolds.py`.
*   **Input:** Standard function calling dataset (Glaive, APIGen).
*   **Logic:**
    1.  Read function name from label.
    2.  Fetch schema.
    3.  Construct string: `{"key": "<MASK>"}`.
    4.  **Self-Adaptive Masking:** Use `<NULL>` tokens for optional fields to handle variable lengths.

**Self-Adaptive Masking with `<NULL>` Tokens - Critical Detail:**

**The Problem:** How many mask tokens do you allocate for variable-length fields?
*   Location field could be "NYC" (3 characters) or "Los Angeles" (11 characters).
*   If you pre-allocate a fixed number of mask tokens, you either waste tokens or truncate long values.

**The Solution:** Use a special `<NULL>` token to indicate "no value" or "unused slot".
*   Allocate a maximum number of mask tokens per field (e.g., 20 tokens).
*   During training, the model learns to use `<NULL>` for unused slots.
*   Example scaffold: `{"location": "<MASK> <NULL> <NULL> <NULL>...", "units": "<MASK> <NULL> <NULL>..."}`
*   Example output: `{"location": "NYC", "units": "<NULL> <NULL> <NULL>..."}`

**Data Processing Steps:**
1.  Take standard function calling example: `{"query": "Weather in London", "tool_call": {"name": "weather", "args": {"loc": "London"}}}`
2.  Look up schema for `weather` function.
3.  Create scaffold template with `<MASK>` and `<NULL>` tokens.
4.  Tokenize the template.
5.  Create `scaffold_mask`: Binary mask with 0 for fixed tokens (keys, brackets, commas) and 1 for `<MASK>` positions.
6.  Create `labels`: Token IDs for actual values ("London") at the mask positions.

### **Phase 3: Training Strategy**
*   **Objective:** Train the Diffusion Head to correctly predict tokens at `<MASK>` positions.
*   **Loss Function:** CrossEntropy on masked tokens only.
*   **Data Mix:**
    *   **Function Calling (100k):** Primary training data.
    *   **Reasoning (20k):** "Replay" data to prevent the model from forgetting how to think (The "Lobotomy" prevention).
    *   **Chat (50k):** To maintain conversational fluency.
*   **Diffusion Steps:** Use **2-4 steps** max. (Iterative refinement is for art; strict constraints need fewer steps).

**Why Include Reasoning Data? Understanding Catastrophic Forgetting (The "Lobotomy Effect"):**

SmolLM3 already knows how to reason. You do NOT need to teach it reasoning from scratch. However, you MUST include reasoning data in your fine-tuning mix for two critical reasons:

1.  **Catastrophic Forgetting Prevention:**
    *   When you fine-tune a small model (3B) heavily on function calling (which is short, JSON-heavy, and rigid), it tends to "forget" how to do long-form, nuanced reasoning.
    *   The model starts trying to output JSON for everything or loses its ability to chain multiple logical steps.
    *   **Solution:** Include reasoning data not to teach new skills, but to "pin the weights" so existing capabilities don't degrade. This is "replay training."

2.  **Domain Adaptation:**
    *   SmolLM3 knows how to solve a math word problem, but it doesn't necessarily know how to reason about YOUR specific API schemas.
    *   You need to adapt its general "thinking" capability to the specific logic of tool selection and parameter extraction.
    *   Example: "The user asked for 'sales data'. I have a `get_sales` tool, but it requires a `date_range`. The user didn't provide a date. I should ask for the date instead of calling the tool with a null value."

**Training Hyperparameters:**

Based on MediaTek paper and schema scaffolding research:

*   **Batch Size:** 48 (per GPU)
*   **Learning Rate:** 1e-4 (initial), with cosine decay
*   **Warmup Steps:** 2,500
*   **Epochs:** 3-4
*   **Optimizer:** AdamW
*   **Weight Decay:** 0.01
*   **Gradient Clipping:** 1.0
*   **Scheduler:** Cosine with linear warmup

**Diffusion-Specific Settings:**

*   **Number of Denoising Steps:** 4 for training accuracy, 2 for inference speed
*   **Noise Schedule:** Linear (uniform masking rate across steps)
*   **Refinement Strategy:** Conservative refinement - keep 70% of previous predictions, update 30% per step
    *   This prevents the model from drastically changing predictions between steps
    *   Ensures stability in structured output generation

**Loss Function Composition:**

The total loss combines three objectives:
1.  **Decision Loss:** Binary cross-entropy for `<|answer|>` vs `<|use_tool|>` classification
2.  **Diffusion Loss:** Cross-entropy on masked token predictions only (not on fixed scaffold tokens)
3.  **Constraint Loss:** Penalizes predictions that violate schema rules (e.g., invalid enum values)

**Loss Weights:** α=1.0 (decision), β=2.0 (diffusion - primary objective), γ=0.5 (constraint - auxiliary)

### **Phase 4: Inference Pipeline**
Implement the "Handshake":
1.  AR Model outputs `<|tool_name:weather|>`.
2.  Code pauses AR generation.
3.  Python generates scaffold `{"loc": <MASK>}`.
4.  Diffusion Head fills `<MASK>` -> `"London"`.
5.  Python combines result -> `{"loc": "London"}`.

---

## 5. Technical "Do's and Don'ts"

### **✅ DO:**
*   **Use SmolLM3-3B:** It is SOTA for 3B and has dual-mode logic built-in.
*   **Use Python for Structure:** Never let the LLM generate brackets or commas. Python generates the Scaffold; LLM generates the values.
*   **Use Decision Tokens:** Force the model to explicitly output `<|use_tool|>` or `<|answer|>` before generating text.
*   **Freeze the Base:** Only train the adapter/diffusion head to save VRAM and preserve general IQ.
*   **Generate Synthetic Data:** Use a teacher model (Qwen3-32B) to generate reasoning traces if you lack data.

### **❌ DO NOT:**
*   **Do NOT use Full LaDiR:** The VAE/Latent space overhead is unnecessary for structured text.
*   **Do NOT use TiDAR:** It optimizes throughput, not latency.
*   **Do NOT use CoT for Tools:** Reasoning traces hurt function calling accuracy.
*   **Do NOT train from scratch:** Fine-tune/Adapt only.
*   **Do NOT use 50+ Diffusion Steps:** It destroys latency. 2-4 steps are sufficient for constrained generation.

---

## 6. Evaluation Benchmarks
Target metrics to define SOTA status for a 3B model:

| Metric | Target | Context |
| :--- | :--- | :--- |
| **MT-Bench** | ~5.3 | Maintenance of conversational quality. |
| **BFCL (AST)** | 82-87% | Structural accuracy (should be high due to scaffolding). MediaTek achieved 85.25% with 7B model. |
| **Relevance Detection** | 65-75% | Ability to know *when* to use a tool (via Decision Token). MediaTek achieved 65.42%. |
| **Hallucination Rate** | <8% | Greatly reduced by schema constraints. |
| **Latency** | <100ms | Avg combined latency (Router + AR + Diffusion). |

**Latency Breakdown by Mode:**

Different modes have different performance characteristics. The overall latency depends on mode distribution:

| Mode | Latency Target | Distribution | Notes |
| :--- | :--- | :--- | :--- |
| **Chat** | 20-30ms | 60% of queries | Standard AR generation, fastest path |
| **Think** | 80-120ms | 15% of queries | Longer due to reasoning trace generation |
| **Tool** | 60-100ms | 25% of queries | Includes decision (10ms) + diffusion (50-90ms) |
| **Router** | <10ms | 100% of queries | Lightweight classifier overhead |

**Weighted Average Latency:**
- (0.60 × 25ms) + (0.15 × 100ms) + (0.25 × 80ms) + 10ms = **45-55ms** typical per request

This is significantly faster than pure diffusion models (which would be 100ms+ for all queries) while maintaining quality.

---

## 7. Code Resources & Inspiration

**What to Steal from Each Repository:**

### **LaDiR - Latent Diffusion Reasoning**
*   **Repository:** [github.com/mk322/LaDiR](https://github.com/mk322/LaDiR)
*   **What to Learn:**
    *   How they implement latent diffusion conditioned on LLM hidden states
    *   Block-wise diffusion logic for sequential generation
    *   Time embedding structure (sinusoidal or learned embeddings)
    *   How they integrate diffusion training with a frozen LLM base
*   **Key Files to Examine:**
    *   `model/diffusion.py` - Core diffusion head architecture
    *   `train.py` - How they integrate diffusion training with frozen LLM
    *   `utils/masking.py` - Masking strategies (directly applicable to scaffolding)
*   **What NOT to Use:**
    *   The VAE/Latent space complexity - unnecessary for structured text
    *   50+ denoising steps - too slow for your use case

### **MDLM - Masked Diffusion Language Models**
*   **Repository:** [github.com/s-sahoo/mdlm](https://github.com/s-sahoo/mdlm)
*   **Paper:** [OpenReview](https://openreview.net/pdf?id=L4uaAR4ArM)
*   **What to Learn:**
    *   Clean PyTorch implementation of masked discrete diffusion
    *   How to train models to denoise masked tokens (exactly what you need)
    *   No VAE complexity - pure discrete tokens (matches your scaffolding approach)
    *   Rao-Blackwellized objective - simple and effective loss function
*   **Key Concept:**
    *   Loss is a mixture of masked language modeling losses at different timesteps
    *   Directly adaptable for your `<MASK>` token filling
*   **What to Use:**
    *   The masking loss objective (cross-entropy on masked positions only)
    *   Simple residual block structure for denoising network

### **Discrete Diffusion Forcing (D2F)**
*   **Repository:** [github.com/zhijie-group/Discrete-Diffusion-Forcing](https://github.com/zhijie-group/Discrete-Diffusion-Forcing)
*   **Paper:** [arXiv:2508.09192](https://arxiv.org/abs/2508.09192)
*   **What to Learn:**
    *   How to make diffusion generation fast (2.5× speedup over AR)
    *   Block-wise generation techniques (maps to function call structure)
    *   Asymmetric distillation for inference speedup (if you want to optimize later)

### **PyTorch Diffusion Tutorial - Practical Implementation**
*   **Repository:** [github.com/priyammaz/PyTorch-Adventures](https://github.com/priyammaz/PyTorch-Adventures)
*   **Video Tutorial:** 2.5-hour step-by-step tutorial on building diffusion LLMs from scratch
*   **What to Learn:**
    *   Covers both pretraining and SFT (supervised fine-tuning)
    *   Very beginner-friendly, step-by-step implementation
    *   Best learning resource if you're new to diffusion models
    *   Shows complete training loop structure

### **SmolLM3 - Base Model & Training Recipes**
*   **HuggingFace:** [SmolLM3 Collection](https://huggingface.co/collections/HuggingFaceTB/smollm3-67263566591d7c6b7e85c14f)
*   **Blog:** [SmolLM3 Announcement](https://huggingface.co/blog/smollm3)
*   **What to Learn:**
    *   Dual-mode reasoning architecture (`think` vs `no_think`)
    *   Synthetic reasoning data generation using teacher models
    *   Training recipe: 11T base + 140B reasoning + 8B SFT tokens
    *   Balanced data mixture strategies (1B non-reasoning + 0.8B reasoning)
    *   Mode-specific training with loss masking on user turns

---

## 8. Implementation Timeline

**Day-by-Day Breakdown for First Week:**

**Day 1: Validation**
*   Set up the `HybridSmolLM` class
*   Instantiate it with dummy inputs
*   Ensure dimensions match between base model and diffusion head
*   Verify forward pass works without errors

**Day 2: Data Pipeline**
*   Write `generate_scaffolds.py`
*   Take 100 examples from Glaive dataset
*   Convert them into `(input_ids, scaffold_mask, labels)` format
*   Verify scaffold masks correctly mark `<MASK>` positions

**Day 3: Training Loop**
*   Write simple PyTorch training loop (using `accelerate` recommended)
*   Feed batch through model
*   Sample random timestep `t` (0 to 3)
*   Forward pass → Backward pass
*   Update ONLY `diffusion_head` parameters (verify base model frozen)

**Day 4: Inference Test**
*   Write inference script
*   Manually create a prompt + scaffold
*   Run diffusion head for t=3, t=2, t=1, t=0 (iterative refinement)
*   Check if it fills in the blank correctly

**Week 2-3: Full Training**
*   Scale to 10k examples, verify no bugs
*   Scale to full 100k function calling + 20k reasoning + 50k chat
*   Monitor losses: decision, diffusion, constraint
*   Run validation on BFCL benchmark every 5k steps

**Week 4: Evaluation & Iteration**
*   Run full BFCL evaluation
*   Measure latency across all modes
*   Identify failure cases
*   Iterate on data or hyperparameters

**Total Timeline:** 4-6 weeks to SOTA small model with this approach.

---

## 9. Realistic Expectations & Resource Requirements

**Performance Targets (Based on Current SOTA for 3B Models):**

| Task | Target | Baseline Reference | Confidence |
| :--- | :--- | :--- | :--- |
| **MT-Bench** | 5.2-5.5 | SmolLM3 baseline ~5.3 | High |
| **GSM8K** | 55-65% | SmolLM3 reasoning + synthetic data | High |
| **BFCL AST** | 82-87% | MediaTek 7B achieved 85.25% | High |
| **BFCL Relevance** | 65-75% | MediaTek + schema scaffolding | Medium |
| **Hallucination (FC)** | <8% | Schema scaffolding proven effective | High |
| **Avg Latency** | 45-65ms | Weighted by mode distribution | High |

**Resource Requirements:**

*   **Training Data:**
    *   100k function calling examples (Glaive + APIGen)
    *   20k reasoning examples (synthetic generation via Qwen3-32B or GPT-4)
    *   50k chat examples (Open ORCA, ShareGPT)
    *   10k negative function calling examples (generated via MediaTek approach)

*   **Compute:**
    *   Single 80GB GPU: 5-7 days training time
    *   8× GPUs: 1-2 days training time
    *   Estimated cost: $500-1000 on cloud GPUs

*   **Inference:**
    *   Deployable on single GPU or edge devices
    *   Manageable memory footprint (~6-8GB for 3B model + diffusion head)

**Critical Success Factors:**

1.  **Start with frozen SmolLM3-3B** - Leverage existing capabilities
2.  **Generate high-quality synthetic data** - Use strong teacher model
3.  **Monitor all three losses** - Decision, diffusion, and constraint
4.  **Validate continuously on BFCL** - Catch issues early
5.  **Use schema scaffolding inference-only** - No additional training needed after SFT

**Biggest Risks:**

*   **Debugging complexity:** Combining three components (router, AR, diffusion) makes debugging painful. Start simple - get each mode working independently before integration.
*   **Data quality:** Garbage in, garbage out. Invest time in high-quality synthetic reasoning data generation.
*   **Overfitting on function calling:** Monitor conversational quality (MT-Bench) throughout training to catch catastrophic forgetting early.

**When This Approach Makes Sense:**

*   ✅ You need a small (<5B), deployable conversational agent
*   ✅ Function calling is important but not the primary use case
*   ✅ You can tolerate 60-100ms latency for function calls
*   ✅ You have access to 50-100k function calling examples

**When to Consider Alternatives:**

*   ❌ You need <50ms latency for ALL queries → Use pure AR + Decision Token only
*   ❌ You need >90% BFCL accuracy → Use larger models (7B+) or specialized FC models
*   ❌ You lack training data → Focus on data generation first before custom architecture