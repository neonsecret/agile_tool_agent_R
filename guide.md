# Project Guide: Hybrid Diffusion-Autoregressive Architecture for Function Calling

**Target Model:** ~3B Parameters (Based on SmolLM3)
**Core Goal:** Create a small, deployable agent that excels at structured function calling (via Diffusion) while
maintaining high-quality conversation and reasoning (via Autoregression).

---

## 1. Executive Summary & Core Concept

The architecture uses a **Dual-Mode Hybrid system** where different computational mechanisms handle different tasks.

* **Conversation & Reasoning:** handled by the **Autoregressive (AR)** base model (SmolLM3).
* **Function Calling:** handled by a **Schema-Constrained Diffusion Head** using a "Scaffolding" technique.
* **Mode Selection:** SmolLM3's native tool-calling capability handles mode routing (no separate classifier needed).

**Why this wins:**

1. **AR** is fast for chat.
2. **Diffusion** allows bidirectional attention, ensuring global consistency in JSON structures.
3. **Scaffolding** (generated by Python, not the LLM) guarantees 0% structural hallucination.

---

## 2. Key Research & Literature

### **A. Function Calling Optimization**

* **Source:** [MediaTek - Enhancing Function-Calling (arXiv:2412.01130v2)](https://arxiv.org/html/2412.01130v2)
* **Key Takeaway:** Do *not* use Chain-of-Thought (CoT) for function calling; it adds noise. Use CoT only for
  math/logic.

### **B. Structured Generation via Diffusion**

* **Source:** [Unveiling Potential of Diffusion LLMs (arXiv:2507.04504)](https://arxiv.org/pdf/2507.04504.pdf)
* **Key Takeaway:** **Schema Scaffolding**. Instead of generating raw JSON, generate a template with masks.
* **Stat:** 65% improvement in structural adherence; 17% reduction in hallucination.

### **C. Base Model Architecture**

* **Source:** [SmolLM3 Blog](https://huggingface.co/blog/smollm3)
* **Key Takeaway:** Already supports dual-mode (`think` vs `no_think`) and native tool calling.

### **D. Code Inspiration**

* **MDLM:** [github.com/s-sahoo/mdlm](https://github.com/s-sahoo/mdlm) - Noise schedule, diffusion mechanics
* **dLLM-CtrlGen:** Schema scaffolding, S3 generation loop (integrated into this project)

---

## 3. The Architecture

### **The Flow**

1. **Input:** User Query + Context + Available Tools.
2. **Stage 1: Mode Decision (Base Model)**
    * The frozen base model decides whether to use a tool or respond with chat.
    * Uses SmolLM3's native tool-calling protocol (tools injected via chat template).
3. **Stage 2: Execution (Mode Dependent)**
    * **If Chat:** Standard AR generation from base model.
    * **If Tool:**
        * **A. Tool Selection:** Base model generates function name via `<tool_call>` format.
        * **B. Scaffold (Python):** Python looks up schema and builds template:
          `{"location": "<MASK>", "unit": "<MASK>"}`.
        * **C. Diffusion (Head):** Trained diffusion head fills *only* the masked tokens (2-4 steps).

### **Step-by-Step Execution**

**Example Query:** "What's the weather in Tokyo?"

1. **Base Model (AR):** Processes query, outputs `<tool_call>{"name": "get_weather", ...`
2. **Python Script:** Reads tool name, looks up schema, generates scaffold with `<MASK>` tokens
3. **Diffusion Head:** Fills masks using bidirectional attention (2-4 steps)
4. **Python Script:** Merges scaffold + predictions → valid JSON output

**Why This Works:**

* **0% Syntax Errors:** JSON structure is generated by Python, not the LLM.
* **0% Schema Hallucination:** Model cannot invent parameters (no mask slots for them).
* **High Accuracy Values:** Diffusion focuses 100% on extracting values, not syntax.

### **Component Breakdown**

| Component          | Type                    | Function                                          | Training           |
|:-------------------|:------------------------|:--------------------------------------------------|:-------------------|
| **Base Model**     | SmolLM3-3B              | Context understanding, mode selection, tool names | **FROZEN**         |
| **Diffusion Head** | Bidirectional Attention | Fills `<MASK>` slots in scaffolds                 | Train from scratch |
| **Python Logic**   | Script                  | Builds JSON structure deterministically           | N/A                |

---

## 4. Implementation Details

### **Project Structure**

```
smollm-diffusion-agent/
├── data/
│   ├── schema.py                # Schema scaffolding with NULL token support
│   ├── dataset_loader.py        # Dataset wrapper with scaffold generation
│   ├── config_utils.py          # Device-aware configuration
│   ├── device_utils.py          # Device detection
│   ├── budget_utils.py          # Field budget calculation
│   ├── metrics.py               # Training metrics (NULL tokens, field accuracy)
│   └── smollm3_prompting.py     # Chat template utilities
├── model/
│   ├── diffusion_head.py        # Schema-constrained diffusion head
│   ├── noise_schedule.py        # LogLinearNoise (from mdlm)
│   ├── attention_blocks.py      # Bidirectional attention blocks
│   └── hybrid_model.py          # Wrapper combining SmolLM + Diffusion
├── train.py                     # Training loop with accelerate
├── inference.py                 # S3 generation pipeline
├── inference_diffusion.py       # Diffusion operations
├── inference_utils.py           # Helper functions
├── config.yaml                  # Configuration
└── tests/                       # Test suite
```

### **Self-Adaptive Masking with NULL Tokens**

**Problem:** Variable-length fields (e.g., "NYC" vs "Los Angeles") require flexible token allocation.

**Solution:** Use `<NULL>` tokens for unused slots:

* Allocate a fixed budget per field (e.g., 32 tokens max)
* All positions get `<MASK>` tokens initially
* Model learns to predict actual values OR `<NULL>` for unused positions
* During inference, `<NULL>` tokens are stripped from output

**Example:**

```
Scaffold: {"location": "<MASK><MASK><MASK>...(32 tokens)"}
Labels:   ["Tokyo", <NULL>, <NULL>, ...]
Output:   {"location": "Tokyo"}
```

---

## 5. Training Configuration

### **Loss Function**

The diffusion head uses a single loss with NULL token handling:

```python
# Cross-entropy on masked positions only
loss = CrossEntropy(predictions[scaffold_mask], labels[scaffold_mask])
```

**NULL Token Handling (prevents excessive NULL predictions):**

1. **`null_loss_weight: 0.1`** - Down-weights loss for positions that should be NULL
    - Prevents model from over-optimizing for NULL predictions

2. **`null_prediction_penalty: 0.3`** - Penalizes predicting NULL on non-NULL positions
    - Adds penalty when model incorrectly outputs NULL for real content
    - `penalty = mean(P(NULL) for non-NULL positions) * 0.3`

3. **`label_smoothing: 0.1`** - Reduces overconfidence, improves generalization

### **Hyperparameters**

| Parameter             | Value | Notes                               |
|:----------------------|:------|:------------------------------------|
| **Batch Size**        | 8     | Per GPU, with gradient accumulation |
| **Learning Rate**     | 1e-4  | With cosine decay                   |
| **Warmup Steps**      | 2500  | Capped at 10% of total steps        |
| **Epochs**            | 8     | Adjust based on dataset size        |
| **Optimizer**         | AdamW |                                     |
| **Gradient Clipping** | 1.0   |                                     |

### **Diffusion-Specific Settings**

| Parameter           | Value          | Notes                               |
|:--------------------|:---------------|:------------------------------------|
| **Training Steps**  | 4              | Num denoising steps during training |
| **Inference Steps** | 4              | Configurable (2-4 recommended)      |
| **Noise Schedule**  | LogLinearNoise | From mdlm                           |
| **Hidden Dim**      | 2048           | Matches base model                  |
| **Num Layers**      | 4              | Bidirectional attention blocks      |
| **Num Heads**       | 8              | Attention heads                     |

### **Dynamic Budget Configuration**

```yaml
data:
  mask_budget: 32          # Max tokens per field
  dynamic_budget:
    enabled: true
    min_tokens: 0
    extra_tokens: 2        # Add buffer for tokenization variance
    max_tokens: 32
```

---

## 6. Inference Pipeline

### **Generation Flow**

```python
from inference import FunctionCallGenerator

generator = FunctionCallGenerator(model, tokenizer, device)

result = generator.generate_unified(
    prompt="What's the weather in Tokyo?",
    tool_registry={"get_weather": weather_schema},
    config=GenerationConfig(steps=4)
)
# Returns: {"mode": "tool", "tool_name": "get_weather", "tool_call": {...}}
```

### **Top-K Remasking Strategy**

During diffusion, the S3 loop uses adaptive token revelation:

1. Predict all masked positions
2. Calculate confidence (log-probabilities)
3. Reveal top-K highest confidence predictions
4. Repeat until all masks filled or steps exhausted

**Budget calculation:** `k = ceil(total_masks / num_steps)`

### **Optimizations**

* **Hidden States Caching:** Base model forward pass runs once; hidden states reused across diffusion steps
* **CUDA Graphs:** Captures diffusion head for reduced kernel launch overhead (CUDA only)
* **torch.compile:** JIT compilation for faster execution (optional)

---

## 7. Evaluation Benchmarks

| Metric                 | Target   | Notes                         |
|:-----------------------|:---------|:------------------------------|
| **BFCL (AST)**         | 82-87%   | Structural accuracy           |
| **Hallucination Rate** | <8%      | Reduced by schema constraints |
| **MT-Bench**           | ~5.3     | Conversational quality        |
| **Latency (Tool)**     | 60-100ms | Including diffusion           |
| **Latency (Chat)**     | 20-30ms  | AR generation only            |

---

## 8. Platform Support

| Device   | Training    | Inference | Quantization         | Optimizations                       |
|----------|-------------|-----------|----------------------|-------------------------------------|
| **CUDA** | ✅ Fast      | ✅ Fast    | 4-bit (bitsandbytes) | unsloth, CUDA graphs, torch.compile |
| **MPS**  | ⚠️ Slow     | ✅ Good    | ❌ (bfloat16)         | torch.compile (2.1+)                |
| **CPU**  | ❌ Very Slow | ❌ Slow    | ❌                    | None                                |

Configuration is auto-adjusted via `data/config_utils.py`:

- Disables 4-bit quantization on MPS/CPU
- Disables CUDA graphs on non-CUDA devices
- Handles unsloth availability

---

## 9. Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Validate setup
python validate_setup.py

# Run tests
pytest tests/ -v -m "not slow"

# Train
python train.py

# Inference
python demo_inference.py
```

---

## 10. Technical Notes

### **✅ Design Decisions:**

* **Use SmolLM3's native tool-calling** instead of a separate router/decision head
* **Freeze base model** - only train diffusion head to preserve general capabilities
* **Python generates JSON structure** - LLM only predicts semantic values
* **Bidirectional attention** in diffusion head for global constraint verification

### **❌ What We Avoided:**

* VAE/Latent space complexity (unnecessary for structured text)
* 50+ diffusion steps (2-4 sufficient for constrained generation)
* Chain-of-Thought for function calling (adds noise, hurts accuracy)
* Separate router classifier (base model handles this natively)
